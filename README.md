# Waivern Compliance Tool (WCT)

A modern, analyser-based compliance analysis framework for detecting and analysing compliance issues across tech stacks that integrates directly into your team's CI/CD pipeline. The open standards that WCT defines for rulesets and analysers, in addition to the Apache-license open source approach for data connectors and the framework itself, means it can be made to work for any technology compliance regulation (GDPR, ePrivacy, EU AI Act, NIS2, DORA, etc.) The output files generated by WCT can be shared with compliance teams to give them everything they need to complete their work, without the hassle for the engineering/product team of filling in questionnaires and spreadsheets. The files can also be used as context for prompts to LLMs to generate drafts of the required documentation under the regulatory framework in question, e.g. RoPA documents, DPIAs, LIAs for GDPR.

## Overview

WCT provides a flexible architecture for compliance analysis through:

- **Connectors**: Extract data from various sources (files, databases, web applications)
- **Analysers**: Perform compliance analysis on extracted data
- **Rulesets**: Rule patterns for compliance static analysis
- **Executor**: Manages the execution pipeline and data flow

The system is designed to be extensible and configurable through YAML runbook files with comprehensive validation that ensures type safety and component interoperability.

ðŸ“‹ **[Development Roadmap](docs/development_roadmap.md)** - See our current progress and planned features

ðŸš€ **Ready to contribute?** Check out our [contribution opportunities](docs/development_roadmap.md#contribution-opportunities) and browse [good first issues](https://github.com/waivern-compliance/waivern-compliance/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) to get started!

## Quick Start

### Installation

This project uses `uv` for dependency management. `uv` can be [installed](https://github.com/astral-sh/uv?tab=readme-ov-file#installation) as a standalone tool or via [`homebrew`](https://formulae.brew.sh/formula/uv).

Once you have `uv` installed, run the below commands:

```bash
# Install all dependencies
uv sync

# Install with development tools
uv sync --group dev

# Install pre-commit hooks (recommended)
uv run pre-commit install
```

**Dependencies**:
- All connector and analyser dependencies are included by default

**Development Dependencies** (`--group dev`):
- Testing and code quality tools

### Basic Usage

1. **Run analysis with a runbook**:
   ```bash
   # Simple file analysis demonstration
   uv run wct run runbooks/samples/file_content_analysis.yaml

   # Comprehensive LAMP stack analysis
   uv run wct run runbooks/samples/LAMP_stack_lite.yaml
   ```

2. **List available components**:
   ```bash
   uv run wct ls-connectors
   uv run wct ls-analysers
   ```

3. **Validate a runbook**:
   ```bash
   uv run wct validate-runbook runbooks/samples/file_content_analysis.yaml
   ```

The output JSON results will be written to the `./outputs` directory.

### Enabling LLM Validation

To enable AI-powered analysis features, you'll need to configure your API key:

1. **Copy the environment template**:
   ```bash
   cp .env.example .env
   ```

2. **Add your Anthropic API key** to the `.env` file:
   ```bash
   ANTHROPIC_API_KEY=your_api_key_here
   ```

3. **Test the configuration**:
   ```bash
   uv run wct test-llm
   ```

With LLM validation enabled, analysers can provide enhanced accuracy for detecting personal data and processing purposes.

### Runbooks Directory

WCT organises runbook configurations in the `runbooks/` directory with samples organised in `runbooks/samples/`:

- **`runbooks/samples/file_content_analysis.yaml`** - Simple file analysis demonstration using personal data analyser
- **`runbooks/samples/LAMP_stack_lite.yaml`** - Comprehensive example demonstrating file, database, and source code analysis
- **`runbooks/samples/LAMP_stack.yaml`** - Advanced MySQL-based LAMP stack analysis
- **`runbooks/README.md`** - Detailed documentation on runbook usage and creation guidelines

Run sample runbooks:
```bash
# Simple file content analysis
uv run wct run runbooks/samples/file_content_analysis.yaml

# Comprehensive LAMP stack analysis
uv run wct run runbooks/samples/LAMP_stack_lite.yaml

# Run with verbose logging
uv run wct run runbooks/samples/file_content_analysis.yaml -v
```

### Example Runbook

```yaml
name: "Compliance Analysis"
description: "Analyse files and databases for sensitive information"

connectors:
  - name: "filesystem_reader"
    type: "filesystem"
    properties:
      path: "./sample_file.txt"
  - name: "my_database"
    type: "mysql"
    properties:
      host: "localhost"
      user: "dbuser"
      password: "dbpass"
      database: "mydb"
      port: 3306


analysers:
  - name: "content_analyser"
    type: "personal_data_analyser"
    properties:
      pattern_matching:
        ruleset: "personal_data"
        evidence_context_size: "medium"
      llm_validation:
        enable_llm_validation: true

execution:
  - connector: "filesystem_reader"
    analyser: "content_analyser"
    input_schema: "standard_input"
    output_schema: "personal_data_finding"
```

**Key Features**:
- **Comprehensive validation**: Automatic input and output validation with clear error messages
- **Explicit connector-analyser mapping**: Clear data flow specification in execution steps
- **Dynamic schema loading**: Flexible schema file discovery with multiple search paths
- **End-to-end validation**: Full pipeline validation from runbook loading to analysis results
- **All-inclusive dependencies**: All connector and analyser dependencies included by default

## IDE Support

WCT runbooks support JSON Schema validation for enhanced developer experience with real-time validation, autocomplete, documentation on hover, and structure guidance.

```bash
# Validate runbooks with enhanced IDE support
uv run wct validate-runbook runbooks/my-runbook.yaml
```

ðŸ“– **Setup instructions**: [IDE Integration Guide](docs/ide-integration.md)


## Architecture

For a comprehensive understanding of the core concepts and design principles that underpin WCT, see **[WCF Core Concepts](docs/wcf_core_concepts.md)**.

### Validation System

WCT uses a comprehensive validation system with two distinct validation approaches:

**Data Flow Schemas** - Validate runtime data between components:
- **StandardInputSchema**: Common input format for filesystem connectors
- **PersonalDataFindingSchema**: Output format for personal data analysis results
- Uses JSON Schema for cross-component data validation

**Configuration Validation** - Validate runbook configuration files:
- **Runbook**: YAML runbook configuration validation with structural requirements
- Uses Python type annotations for clear validation rules

Key features:
- **Declarative validation**: Clear validation rules with comprehensive error messages
- **Better error messages**: Field path reporting with detailed validation failure information
- **Type safety**: Strongly typed interfaces throughout the system
- **Versioned architecture**: Consistent schema structure in `json_schemas/{name}/{version}/`

### Core Components

- **`src/wct/runbook.py`**: Runbook loading with comprehensive validation
- **`src/wct/executor.py`**: Schema-aware execution engine with automatic data flow matching
- **`src/wct/schemas/`**: Strongly typed schema system (see [schemas README](src/wct/schemas/README.md))
  - Data flow schemas: `StandardInputSchema`, `PersonalDataFindingSchema`
  - JSON schema validation for component data exchange
- **`src/wct/connectors/`**: Data source connectors
  - `filesystem/` - File content extraction
  - `mysql/` - Database analysis
- **`src/wct/analysers/`**: Compliance analysis engines
  - `personal_data_analyser/` - Personal data detection with LLM validation
- **`src/wct/rulesets/`**: Versioned rule patterns for compliance static analysis

### Validation Pipeline Flow

1. **Runbook loading**: Validates YAML structure, field requirements, and patterns
2. **Cross-reference validation**: Verify connector/analyser names and execution step relationships
3. **Execution orchestration**: Automatic schema matching between connector outputs and analyser inputs
4. **Data validation**: Runtime validation of data flowing between components using Message objects
5. **Results processing**: Schema-validated analysis results with comprehensive error reporting

### Modular Architecture Benefits

- **Schema Contracts**: Clear input/output schema declarations for all components
- **Complete Dependencies**: All connector and analyser dependencies included for immediate use
- **Independent Testing**: Each module tested in isolation with comprehensive coverage
- **Declarative Configuration**: YAML runbooks with comprehensive validation eliminate manual validation
- **Type Safety**: Strongly typed interfaces with comprehensive error reporting
- **Compliance Focus**: Components optimised for regulatory requirements (GDPR, CCPA)

### Configuration Format

WCT runbooks use **validated YAML** with mandatory schema specifications:

```yaml
# All fields comprehensively validated
name: "Analysis Pipeline"                    # Required
description: "Compliance analysis setup"    # Required

connectors:                                  # Required, minItems: 1
  - name: "filesystem_reader"               # Required, pattern validated
    type: "filesystem"                      # Required
    properties: {...}                       # Required

analysers:                                  # Required, minItems: 1
  - name: "data_analyser"                  # Required, pattern validated
    type: "personal_data_analyser"         # Required
    properties: {...}                      # Required

execution:                                  # Required, minItems: 1
  - connector: "filesystem_reader"          # Required, cross-referenced
    analyser: "data_analyser"              # Required, cross-referenced
    input_schema: "standard_input"    # Required
    output_schema: "personal_data_finding"  # Required
```

## Development

### Development Commands

**Testing**:
```bash
uv run pytest                    # Run all tests
uv run pytest -v                 # Verbose output
uv run pytest tests/specific.py  # Run specific test
```

**Code Quality** (Package-Centric Architecture):

This monorepo uses package-centric quality checks where each package owns its configuration and can be checked independently:

```bash
# Workspace-level (checks all packages)
./scripts/lint.sh               # Lint all packages
./scripts/format.sh             # Format all packages
./scripts/type-check.sh         # Type check all packages
./scripts/dev-checks.sh         # Run all checks + tests

# Package-level (check individual packages)
cd apps/wct && ./scripts/lint.sh
cd libs/waivern-core && ./scripts/type-check.sh

# Pre-commit hooks (automatic on commit)
uv run pre-commit install         # Install hooks (once)
uv run pre-commit run --all-files # Run manually
```

**Logging Options**:
All WCT commands support detailed logging:
```bash
uv run wct run runbooks/sample_runbook.yaml --log-level DEBUG
uv run wct run runbooks/sample_runbook.yaml -v  # Shortcut for debug
```

### Extending WCT

#### Creating a Schema-Compliant Connector

```python
from typing import Any
from typing_extensions import Self, override
from wct.connectors.base import Connector
from wct.schema import WctSchema

class MyConnector(Connector[dict[str, Any]]):
    @classmethod
    @override
    def get_name(cls) -> str:
        return "my_connector"

    @classmethod
    @override
    def from_properties(cls, properties: dict[str, Any]) -> Self:
        return cls(**properties)

    @override
    def extract(self, schema: WctSchema[dict[str, Any]]) -> dict[str, Any]:
        # Extract and transform data to match schema
        return {"data": "schema_compliant_content"}

    @override
    def get_output_schema(self) -> WctSchema[dict[str, Any]]:
        return WctSchema(name="my_data", type=dict[str, Any])
```

#### Creating a Schema-Aware Analyser

```python
from typing import Any
from typing_extensions import Self, override
from wct.analysers.base import Analyser
from wct.schema import WctSchema
from wct.message import Message

class MyAnalyser(Analyser):
    @classmethod
    @override
    def get_name(cls) -> str:
        return "my_analyser"

    @classmethod
    @override
    def from_properties(cls, properties: dict[str, Any]) -> Self:
        return cls(**properties)

    @override
    def process_data(self, message: Message) -> Message:
        # Process schema-validated input message
        # Extract data from the message
        input_data = message.content

        # Perform your analysis logic here
        findings = ["compliance_issue_1"]

        # Create result data
        result_data = {"findings": findings}

        # Return new Message with results
        return Message(
            id=f"Analysis results for {message.id}",
            content=result_data,
            schema=self.get_output_schema(),
        )

    @override
    def get_input_schema(self) -> WctSchema[dict[str, Any]]:
        return WctSchema(name="my_data", type=dict[str, Any])

    @override
    def get_output_schema(self) -> WctSchema[dict[str, Any]]:
        return WctSchema(name="my_results", type=dict[str, Any])

```

**Key Analyser Features**:
- **Message-Based Architecture**: All analysers now work with `Message` objects for unified data flow
- **Automatic Validation**: The `process()` method automatically validates both input and output Messages
- **Schema-Aware Processing**: Input and output Messages are validated against declared schemas
- **Type Safety**: Full type checking with generic type parameters and Message containers
- **Error Handling**: Comprehensive error messages for validation failures
- **Seamless Processing**: Implement `process_data()` with Message objects, validation is handled transparently
- **No Manual Validation**: Analysers no longer need to implement validation - handled by the Message mechanism

### Project Structure

```
src/wct/
â”œâ”€â”€ __main__.py           # CLI entry point
â”œâ”€â”€ runbook.py           # Runbook loading and validation
â”œâ”€â”€ executor.py          # Schema-aware execution engine
â”œâ”€â”€ schemas/             # Strongly typed schema system
â”‚   â”œâ”€â”€ README.md        # Schema architecture and usage documentation
â”‚   â”œâ”€â”€ standard_input.py # StandardInputSchema (data flow validation)
â”‚   â””â”€â”€ json_schemas/    # Versioned JSON Schema definitions
â”‚       â””â”€â”€ standard_input/1.0.0/
â”‚           â””â”€â”€ standard_input.json # JSON Schema validation rules
â”œâ”€â”€ connectors/          # Data source extractors
â”‚   â”œâ”€â”€ base.py         # Abstract connector interface
â”‚   â”œâ”€â”€ filesystem/     # File content extraction
â”‚   â””â”€â”€ mysql/          # Database analysis
â”œâ”€â”€ analysers/          # Compliance analysis engines
â”‚   â”œâ”€â”€ base.py         # Abstract analyser interface
â”‚   â””â”€â”€ personal_data_analyser/ # Personal data detection with LLM
â””â”€â”€ rulesets/           # Rule patterns for compliance static analysis
    â”œâ”€â”€ data/           # Versioned rule pattern configurations
    â”œâ”€â”€ base.py         # Ruleset framework
    â””â”€â”€ {ruleset}.py    # Rule pattern loaders

runbooks/               # YAML configuration files
â”œâ”€â”€ README.md          # Usage guidelines and examples
â””â”€â”€ samples/           # Sample runbook configurations
    â”œâ”€â”€ file_content_analysis.yaml  # Basic file analysis demo
    â””â”€â”€ LAMP_stack.yaml             # Comprehensive analysis example
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run quality checks: `./scripts/dev-checks.sh` (runs all checks + tests)
5. Submit a pull request

### Code Standards

This project follows a **package-centric quality architecture**:
- Each package (`apps/wct/`, `libs/waivern-core/`) owns its quality tool configuration
- Type annotations required (basedpyright in strict mode)
- Code formatting with ruff
- Security checks with bandit
- Comprehensive test coverage (737+ tests)

Run `./scripts/dev-checks.sh` before committing to ensure all standards are met.

## License

This project is licensed under the **Apache License 2.0**. See the [`LICENSE`](LICENSE) file for full details.

## Support

### Community Support

Join our Discord community for support, discussions, and updates:

ðŸ”— **[Discord Server](https://discord.gg/wj2RV4zUYM)**

Our Discord server provides:
- **General Help** - Get assistance with installation, configuration, and usage
- **Development Discussion** - Collaborate on new features and improvements
- **Bug Reports** - Report issues and get community support
- **Feature Requests** - Suggest and discuss new functionality
- **Community Showcase** - Share your WCT implementations and use cases

### Other Support Channels

- **GitHub Issues** - For bug reports and feature requests: [Open an Issue](https://github.com/waivern-compliance/waivern-compliance/issues)
- **GitHub Discussions** - For general questions and community discussions
- **Documentation** - Check the [README](README.md) and [CLAUDE.md](CLAUDE.md) for detailed guidance
